---
layout: page

title: ðŸ¤–ðŸ¦¾ Computer Vision System for Industrial Bin Picking Robot Arm 
description: "Computer Vision System for Bin Picking Task using image Segmentation models such as SAM, DeepLab, Unet, Segnet and attempt to implement in an industrial robot arm using ROS"
# img: assets/img/sp_cup_2024/sp_cup_denoising.png
importance: 1
category: Ongoing
related_publications: false

date: 2022-10-05T14:08:51+05:30
draft: false
tags : ['ROS',"PyTorch", "Image Processing", "Edge Computing"]
github: https://github.com/mora-bprs/SAM-model
website: https://mora-bprs.github.io/

---
Please visit the [Project Web Page](https://mora-bprs.github.io/) for this project documentation. This website will be updated by 10th of June 2024 on the Evaluation Day with the detailed documentation. 

In this project, our assigned task was to `build a computer vision system to get the coordinates of a single box given an image using a segmentation method`. 

This task is a simplified version of [Bin Picking](https://en.wikipedia.org/wiki/Bin_picking) which is a core problem in `computer vision` and `robotics`. 

___

## DeepLab Model 

[Github](https://github.com/mora-bprs/DeepLab) 

In this first attempt, the team along with my group mate, Isiri Withanawasam was asked to refer to `Semantic Segmentation model : DeepLab` paper and implement the model for our application.
 
Since the DeepLab model is not trained on any segmentation dataset which contains box images, a dataset of about 100 annotated images is created. 

<div class="container">
  <div class="row">
    <div class="col-sm">
      <div class="row">

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/created_original_dataset.png" title="Created dataset  " class="img-fluid rounded z-depth-1" %}
          <div class="caption">Created box image dataset </div>
        </div>

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/created_annotated_dataset.png" title="Annotated dataset" class="img-fluid rounded z-depth-1" %}
          <div class="caption">Annotated dataset</div>
        </div>

      </div>
    </div>
  </div>
</div>

Then the DeepLabV3 model was trained on our custom dataset using transfer learning. This is the result we got after 25 epochs. 

<div class="container">
  <div class="row">
    <div class="col-sm">
      <div class="row">

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/sample_input_image.png" title="sample_input_image " class="img-fluid rounded z-depth-1" %}
          <div class="caption">Sample input iamge</div>
        </div>

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/sample_output_image_transfer_learning.png" title="sample_output_image_transfer_learning.png:Zone" class="img-fluid rounded z-depth-1" %}
          <div class="caption">Sample output image after finetuning DeepLabV3 model</div>
        </div>

      </div>
    </div>
  </div>
</div>

Since we were not satisfied with the results, we found the paper SAM model and moved onto using it according to the advice by our supervisor. 

___

## SAM Model

[Github](https://github.com/mora-bprs/SAM-model)

Using this model (Segment Anything by MetaAI), we could get the desired output we were looking for. The results were taken in Google Colab using a  NVIDIA Tesla T4 GPU. 

<!-- Add SAM model result here -->
<div class="container">
  <div class="row">
    <div class="col-sm">
      <div class="row">

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/sam_0.png" title="sample_input_image " class="img-fluid rounded z-depth-1" %}
          <div class="caption">sample mask generated by SAM model</div>
        </div>

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/sam_box_1.jpg" title="sample_output_image_transfer_learning.png:Zone" class="img-fluid rounded z-depth-1" %}
          <div class="caption">Desired output from SAM model</div>
        </div>

      </div>
    </div>
  </div>
</div>

___

## Fast SAM Model 

[Github](https://github.com/mora-bprs/SAM-model)

Since we want a real time inference of the model, and our computation to happen on an edge device. Therefore we wanted to use the model with `Fast SAM`. With this model we could get the result even in  edge with less time using CPU compared to the SAM model sacrifising some accuracy and gaining advantage of inference time. 

<!-- Add Fast SAM results here -->

### On Local Machine 
<div class="row">
    <div class="">
        {% include figure.liquid loading="eager" path="assets/img/bin_picking/fast_sam_on_local_pc.png" title="local machine" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Fast SAM inference on Local Machine  (CPU - Intel i5 11 th Gen)
</div>

### On Raspberry PI 4B 
<div class="row">
    <div class="">
        {% include figure.liquid loading="eager" path="assets/img/bin_picking/fast_sam_on_r_pi.png" title="local machine" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Fast SAM inference on Raspeberry PI 4B (CPU - Broadcom BCM2711, Quad core Cortex-A72 (ARM v8))
</div>

Note : The image in Raspberry Pi is not shown because, Ubuntu Server was running in the Raspberry Pi 4B with no GUI. That is the reason for QT error. 

## Camera integration & Multithreading 

To decrease the time for inference we use multithreading to get input imags from the camera in one thread and buffer them. And do the computation in another thread to get the output. See the [latest commit](https://github.com/mora-bprs/SAM-model/commit/b53a6f067fee8f3dfa333111b9a9469a39de5e3d) in `old` branch in the github repository. (not yet merged to main branch) 

___

## Future Improvements 

By the time I am writing this, we have successfully completed the assigned task. As part of the hardware project we have created a custom gripper. 

After the evaluation day, we hope to integrate this vision system and the custom gripper with an industrial robot arm using ROS2, going the extra mile.

We have already checked two robot arms available in our university's Electrical Engineering Department. Here are those robot arms. 

<div class="container">
  <div class="row">
    <div class="col-sm">
      <div class="row">

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/kuka_arm.png" title="sample_input_image " class="img-fluid rounded z-depth-1" %}
          <div class="caption">Kuka KR 6 R900 sixx industrial robot arm</div>
        </div>

        <div class="col">
          {% include figure.liquid loading="eager" path="assets/img/bin_picking/kinova_arm.png" title="sample_output_image_transfer_learning.png:Zone" class="img-fluid rounded z-depth-1" %}
          <div class="caption">Kinova Gen 2 robot arm </div>
        </div>

      </div>
    </div>
  </div>
</div>

Specifications of the above robot arms

- [Kuka KR 6 R900 sixx industrial robot arm](https://www.kuka.com/-/media/kuka-downloads/imported/8350ff3ca11642998dbdc81dcc2ed44c/0000205456_en.pdf)
- [Kinova Gen 2 robot arm](https://www.kinovarobotics.com/product/gen2-robots)

